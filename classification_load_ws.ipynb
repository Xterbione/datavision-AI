{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NHL Stenden Classification Workshop</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Packages: Configuration</h2>\n",
    "The followings imports are required to run the code. Click the run button to import the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Computing Hardware: Configuration</h2>\n",
    "Neural Networks can be executed on different hardware devices. The following code block allows the user to indicate whether to run the Neural Network on the CPU or on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Either run the Neural Network on the CPU (run_cpu = True) or GPU (run_cpu = False)\n",
    "run_cpu = False\n",
    "\n",
    "if run_cpu:\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data: Configuration</h2>\n",
    "The following code downloads and loads the dataset. Transforms are used to resize and normalize the input images.\n",
    "\n",
    "<h3>Exercise</h3>\n",
    "1. <em> Adapt the batch size. What influence does it have on the performance of the model? </em> <br>\n",
    "2. <em> Add a Horizontal Flip to the list of transformations. What influence does it have on the performance of the model? </em> <br>\n",
    "3. <em> Choose any other data augmentation that is listed <a href=\"https://pytorch.org/vision/stable/transforms.html\">here</a>. What influence does it have on the performance of the model? </em> </br>\n",
    "4. <em> Why would you preferably not add data augmentation to the testing set? </em>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Make experiments reproducible\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration (Feel free to experiment with these values)\n",
    "batch_size = 4\n",
    "image_size = 64\n",
    "\n",
    "# Normalizes the images\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Resize((image_size, image_size)),\n",
    "     #transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.Normalize((0.5), (0.5))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Data: Standard Dataset</h2>\n",
    "The following code denotes where the data is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration: Standard dataset\n",
    "data_path = './detectionBanana'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Data: Dataset Division</h2>\n",
    "The following code downloads and loads the dataset.  Three different loaders are used for the training, validation and testing images. This ensures that the images do not overlap across the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'class_to_index' must have at least one entry to collect any samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Dataset division (train, validation and test split)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m valid_extension \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.PNG\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 14\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolderNB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalid_extension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m ImageFolderNB(root\u001b[38;5;241m=\u001b[39mdata_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/valid\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform, is_valid_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m path: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(path)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m valid_extension)\n\u001b[0;32m     16\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m ImageFolderNB(root\u001b[38;5;241m=\u001b[39mdata_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/test\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform, is_valid_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m path: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(path)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m valid_extension)\n",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36mImageFolderNB.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\datasets\\folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m    144\u001b[0m classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m--> 145\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\datasets\\folder.py:189\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\datasets\\folder.py:66\u001b[0m, in \u001b[0;36mmake_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m     64\u001b[0m     _, class_to_idx \u001b[38;5;241m=\u001b[39m find_classes(directory)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m class_to_idx:\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_to_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must have at least one entry to collect any samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m both_none \u001b[38;5;241m=\u001b[39m extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_valid_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     69\u001b[0m both_something \u001b[38;5;241m=\u001b[39m extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_valid_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: 'class_to_index' must have at least one entry to collect any samples."
     ]
    }
   ],
   "source": [
    "# Uploading a file generates unwanted files. This messes up the dataloading part. The code below ensures that we only load images.\n",
    "class ImageFolderNB(torchvision.datasets.ImageFolder):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def find_classes(self, directory):\n",
    "        list_classes = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d)) and d[0] != \".\"]\n",
    "        dict_classes = {d:i for i, d in enumerate(sorted(list_classes))}\n",
    "\n",
    "        return list_classes, dict_classes\n",
    "\n",
    "# Dataset division (train, validation and test split)\n",
    "valid_extension = [\".jpg\", \".png\", \".jpeg\", \".JPG\", \".PNG\"]\n",
    "train_dataset = ImageFolderNB(root=data_path + '/train', transform=transform, is_valid_file = lambda path: os.path.splitext(path)[1] in valid_extension)\n",
    "validation_dataset = ImageFolderNB(root=data_path + '/valid', transform=transform, is_valid_file = lambda path: os.path.splitext(path)[1] in valid_extension)\n",
    "test_dataset = ImageFolderNB(root=data_path + '/test', transform=transform, is_valid_file = lambda path: os.path.splitext(path)[1] in valid_extension)\n",
    "\n",
    "# Define the different dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "\n",
    "classes = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data: Data Science Perspective (Qualitative)</h2>\n",
    "The following code block is responsible for visualizing some images. One batch of images (default = 4) is first normalized between 0 and 1 and then displayed. Feel free to run this block multiple times to display different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the image to be in the range [0, 1] and display one batch\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data: Data Science Perspective (Quantitative)</h2>\n",
    "The following code block is responsible for displaying some statistics of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = {}\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "channel, height, width = images[0].shape\n",
    "\n",
    "print(\"Images have %d channel(s) with height %d and width %d\" % (channel, height, width))\n",
    "\n",
    "for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "    # count the number of instances per class\n",
    "    for label in labels:\n",
    "        if label.item() in train_labels:\n",
    "            train_labels[label.item()] += 1\n",
    "        else:\n",
    "            train_labels[label.item()] = 1\n",
    "\n",
    "print(\"Number of classes: \" + str(len(train_labels.keys())))\n",
    "print(\"Number of instances per class:\")\n",
    "\n",
    "for key in sorted(train_labels):\n",
    "    print(\"%s: %s\" % (classes[key], train_labels[key]))\n",
    "\n",
    "print(\"Accuracy random guessing on train set: \" + str(float(max(train_labels.values()) / int(sum(train_labels.values())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Convolutional Neural Network: Architecture</h2>\n",
    "The following code defines the Convolutional Neural Network (CNN). Several parameters can be adapted for experimentation. The network consists several components including convolutional layers (nn.Conv2d), maximum pooling layers (nn.MaxPool2d), activation functions (F.relu, F.softmax) and fully connected layers (nn.Linear). Whenever you want to reset the model, you can run this cell.\n",
    "\n",
    "<h3>Exercise</h3>\n",
    "1. <em> Change the learning rate. Try to experiment with values between 0.1 and 0.0001. What effect does it have on the training behaviour? Is the model performing better or worse? </em> <br>\n",
    "2. <em> Change the activation functions from F.relu to F.sigmoid. What effect does it have on the model? </em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CNN: Network configuration\n",
    "num_filters_layer_1 = 6\n",
    "num_filters_layer_2 = 16\n",
    "\n",
    "filter_size_layer_1 = 5\n",
    "filter_size_layer_2 = 5\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_classes = len(classes)\n",
    "\n",
    "#output\n",
    "output_conv_dim = int((((image_size - (filter_size_layer_1 - 1)) / 2) - (filter_size_layer_2 - 1)) / 2)\n",
    "\n",
    "# Definition of the Neural Network and its components\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channel, out_channels=num_filters_layer_1, kernel_size=filter_size_layer_1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_filters_layer_1, out_channels=num_filters_layer_2, kernel_size=filter_size_layer_2)\n",
    "        self.fc1 = nn.Linear(num_filters_layer_2 * output_conv_dim * output_conv_dim, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "model = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training a CNN: Loss Function and Optimizer</h2>\n",
    "A loss function is used to compute the discrepancy between the prediction of the network and the desired output. This discrepancy is then minimized by executing an optimization algorithm. The following code block defines the loss function and instantiates the optimizer.\n",
    "\n",
    "<h3>Exercise</h3>\n",
    "1. <em> Change the SGD optimizer to Adam. What difference does it make on the training process and overall performance of the model? </em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "best_validation_loss = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Training a CNN: Forward and Backward Pass</h2>\n",
    "The Neural Network is ready to be trained. The following code block loops once over all training inputs (one epoch) and validates the performance.\n",
    "\n",
    "<h3>Exercise</h3>\n",
    "1. <em> Run the following code blocks multiple times. What happens to the validation loss? What effect does it have on the overall performance of the model? </em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "running_loss_val = 0.0\n",
    "\n",
    "# Training a CNN: Training (Get the inputs; data is a list of [inputs, labels])\n",
    "for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "     # Training a CNN: Optimizer (sets gradients to 0)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Training a CNN: Forward Pass\n",
    "    outputs = net(inputs)\n",
    "    loss = ce_loss(outputs, labels)\n",
    "\n",
    "    # Training a CNN: Backward Pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "print(\"Training loss after \" + str(i) + \" minibatches is: \" + str(running_loss / i))\n",
    "\n",
    "# Training a CNN: Validation (Get the inputs; data is a list of [inputs, labels])\n",
    "for i, (inputs, labels) in enumerate(validloader, 0):\n",
    "\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    # Training a CNN: Forward Pass\n",
    "    outputs = net(inputs)\n",
    "    loss_val = ce_loss(outputs, labels)\n",
    "\n",
    "    running_loss_val += loss_val\n",
    "\n",
    "# Training a CNN: Validation (Store the best model)\n",
    "if running_loss_val < best_validation_loss:\n",
    "    PATH = './best.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    best_validation_loss = running_loss_val\n",
    "\n",
    "print(\"Validation loss after \" + str(i) + \" minibatches is: \" + str(float(running_loss_val) / i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Testing: Data Science Perspective (Qualitative)</h2>\n",
    "Before evaluating the performance of our model on the testing set. It is a good idea to get an impression of the test data. The following code block below visualizes a few test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images.cpu()))\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Testing: Confidence of Prediction</h2>\n",
    "Although the above code provides a prediction for our image. It would be interesting to see how confident the model is about its decision. The below code normalizes using softmax the output of our model. This results in scores for each class that lie between 0 and 1 and sum to 1. Execute the code block below. What can you say about the resulting figure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\n",
    "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return f_x\n",
    "\n",
    "# For visualization purposes, only visualize a maximum of first four predictions of the above images\n",
    "max_confidence_predictions = 4\n",
    "\n",
    "if batch_size < max_confidence_predictions:\n",
    "    max_confidence_predictions = batch_size\n",
    "\n",
    "fig, _ = plt.subplots(nrows=1, ncols=max_confidence_predictions, sharex=True,\n",
    "                                    figsize=(12, 6))\n",
    "\n",
    "fig.suptitle('Probability per class', fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    ax.set_xlabel('Probability', fontsize=10)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Class', fontsize='medium')\n",
    "\n",
    "    if i != 0:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    ax.set_title(\"Label: \" + classes[labels[i]])\n",
    "\n",
    "    num = [classes[i] for i in range(num_classes)]\n",
    "\n",
    "    outputs = net(images)\n",
    "    softmax_output = softmax(outputs[i].data.cpu().numpy())\n",
    "\n",
    "    color = ['grey' if (x < max(softmax_output)) else 'blue' for x in softmax_output]\n",
    "    ax.barh(num,softmax_output, color = color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Testing: Accuracy and Confusion Matrix</h2>\n",
    "The following code block makes predictions on the entire testing set. The overall accuracy and the accuracy per class are computed and displayed below. Additionally, a confusion matrix is constructed to indicate how false positives and false negatives contribute to the accuracy.\n",
    "\n",
    "<h3>Exercise</h3>\n",
    "1.<em> One way to improve the performance of the model is to add more training and/or validation images. Use the upload functionality within Jupyter Notebook to upload additional images (more info <a href=\"https://tljh.jupyter.org/en/latest/howto/content/add-data.html\">here</a>). The dataset is located <a href=\"/tree\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>. Do you observe any improvements in the performance of the model? </em> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# count number of (correct) predictions\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "predicts = []\n",
    "ground_truth = []\n",
    "with torch.no_grad():\n",
    "    for (images, labels) in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "            predicts.append(prediction.cpu().numpy())\n",
    "            ground_truth.append(label.cpu().numpy())\n",
    "\n",
    "total_accuracy = 0\n",
    "\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    total_accuracy += accuracy\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "print(f'Overall accuracy: {total_accuracy/len(correct_pred.items()):.1f} %')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(ground_truth, predicts), display_labels=classes)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
